# 工作指导
# 1.遍历的元素数量应和页面上的标题数量保持一致，可以是a标签或图片，不对遍历的元素做限制，只要能和标题数量一致即可。像"td"这样的可能会导致多遍历的标签尽量不要使用。
# 2.关于详情页链接，因为我们现在不抓详情，所以只要保证详情页的地址针对每一个标题是唯一的即可。
# 3.标题(title): 优先标签的title属性，没有则抓标签内容
# 4.发布日期(publish_date): 标准格式"yyyy-MM-dd"或"MM-dd"，也可以有空格或"年月日"汉字或"/"，但不允许有其他字符。如果列表页没有发布日期则可以不抓删除发布日期相关的配置即可。
# 5.关于抓取频率，如果一个网站发布非常频繁，可以适当缩短抓取频率，既@every(minutes=30)和@config(age=29 * 60)，前者单位是分钟、后者是秒，后者的时间要比前者小1分钟。比如改为5分钟间隔@every(minutes=5)和@config(age=4 * 60)
#
# 常用方法
# 1.获得属性 attr.title
# 2.获得内容 text()
# 3.切片-内容截取 [:]，比如[1:-1]去掉第一个和最后一个字符，[1:]只去掉第一个字符，[:-1]只去掉最后一个字符
# 4.找到指定子元素 .find('xx')
# 5.通过寻找第几个 .eq(index)
# 6.找到父元素 .parent('xx')
# 7.找到同级下一个元素 .next()
# 8.找到同级上一个元素 .prev()
# 9.替换指定字符.replace("发布日期", "")
# 10.去掉空格.strip()
# 11.关于【li > a】与【li a】的区别，比如<li><a id="a1"></a><span><a id="a2"></a></span</li>，前者只会选择到直属的a标签(id1)，后者会选择到li下左右的a标签(id1和id2)
#
# 特殊情况
# 1.用浏览器打开页面有列表，pyspider中web页面没有。解决办法：on_start的crawl方法中追加fetch_type='js'
# 2.抓取返回599。解决办法：on_start的crawl方法中追加validate_cert=False
# 3.列表页内容在原网站</html>标签之后。解决办法：将遍历语句中"response.doc('.secList li a').items()"中的doc改为fulldoc，"response.fulldoc('.secList li a').items()"